{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ishita Kapur, 1001753123\n",
    "#### Data Mining - Assignment 2, Spring 2020\n",
    "\n",
    "\n",
    "## kNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "References:\n",
    "https://stackoverflow.com/questions/18424228/cosine-similarity-between-2-number-lists - for cosine similarity\n",
    "https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/ - for nearest neighbors        \n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal_length  sepal_width  petal_length  petal_width           class\n",
      "0             5.1          3.5           1.4          0.2     Iris-setosa\n",
      "1             4.9          3.0           1.4          0.2     Iris-setosa\n",
      "2             4.7          3.2           1.3          0.2     Iris-setosa\n",
      "3             4.6          3.1           1.5          0.2     Iris-setosa\n",
      "4             5.0          3.6           1.4          0.2     Iris-setosa\n",
      "..            ...          ...           ...          ...             ...\n",
      "145           6.7          3.0           5.2          2.3  Iris-virginica\n",
      "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
      "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
      "148           6.2          3.4           5.4          2.3  Iris-virginica\n",
      "149           5.9          3.0           5.1          1.8  Iris-virginica\n",
      "\n",
      "[150 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#reading data from the csv file\n",
    "data = pd.read_csv('iris.data', header=None, names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class'])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a)\n",
    "Dividing the dataset as development and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development Set:\n",
      "      sepal_length  sepal_width  petal_length  petal_width            class\n",
      "37            4.9          3.1           1.5          0.1      Iris-setosa\n",
      "21            5.1          3.7           1.5          0.4      Iris-setosa\n",
      "16            5.4          3.9           1.3          0.4      Iris-setosa\n",
      "134           6.1          2.6           5.6          1.4   Iris-virginica\n",
      "49            5.0          3.3           1.4          0.2      Iris-setosa\n",
      "..            ...          ...           ...          ...              ...\n",
      "74            6.4          2.9           4.3          1.3  Iris-versicolor\n",
      "2             4.7          3.2           1.3          0.2      Iris-setosa\n",
      "122           7.7          2.8           6.7          2.0   Iris-virginica\n",
      "108           6.7          2.5           5.8          1.8   Iris-virginica\n",
      "137           6.4          3.1           5.5          1.8   Iris-virginica\n",
      "\n",
      "[112 rows x 5 columns] \n",
      "\n",
      "Test Set:\n",
      "      sepal_length  sepal_width  petal_length  petal_width            class\n",
      "82            5.8          2.7           3.9          1.2  Iris-versicolor\n",
      "73            6.1          2.8           4.7          1.2  Iris-versicolor\n",
      "80            5.5          2.4           3.8          1.1  Iris-versicolor\n",
      "67            5.8          2.7           4.1          1.0  Iris-versicolor\n",
      "144           6.7          3.3           5.7          2.5   Iris-virginica\n",
      "107           7.3          2.9           6.3          1.8   Iris-virginica\n",
      "63            6.1          2.9           4.7          1.4  Iris-versicolor\n",
      "7             5.0          3.4           1.5          0.2      Iris-setosa\n",
      "99            5.7          2.8           4.1          1.3  Iris-versicolor\n",
      "125           7.2          3.2           6.0          1.8   Iris-virginica\n",
      "43            5.0          3.5           1.6          0.6      Iris-setosa\n",
      "102           7.1          3.0           5.9          2.1   Iris-virginica\n",
      "59            5.2          2.7           3.9          1.4  Iris-versicolor\n",
      "33            5.5          4.2           1.4          0.2      Iris-setosa\n",
      "58            6.6          2.9           4.6          1.3  Iris-versicolor\n",
      "61            5.9          3.0           4.2          1.5  Iris-versicolor\n",
      "146           6.3          2.5           5.0          1.9   Iris-virginica\n",
      "97            6.2          2.9           4.3          1.3  Iris-versicolor\n",
      "24            4.8          3.4           1.9          0.2      Iris-setosa\n",
      "71            6.1          2.8           4.0          1.3  Iris-versicolor\n",
      "4             5.0          3.6           1.4          0.2      Iris-setosa\n",
      "54            6.5          2.8           4.6          1.5  Iris-versicolor\n",
      "100           6.3          3.3           6.0          2.5   Iris-virginica\n",
      "38            4.4          3.0           1.3          0.2      Iris-setosa\n",
      "119           6.0          2.2           5.0          1.5   Iris-virginica\n",
      "45            4.8          3.0           1.4          0.3      Iris-setosa\n",
      "23            5.1          3.3           1.7          0.5      Iris-setosa\n",
      "126           6.2          2.8           4.8          1.8   Iris-virginica\n",
      "147           6.5          3.0           5.2          2.0   Iris-virginica\n",
      "130           7.4          2.8           6.1          1.9   Iris-virginica\n",
      "41            4.5          2.3           1.3          0.3      Iris-setosa\n",
      "72            6.3          2.5           4.9          1.5  Iris-versicolor\n",
      "135           7.7          3.0           6.1          2.3   Iris-virginica\n",
      "48            5.3          3.7           1.5          0.2      Iris-setosa\n",
      "123           6.3          2.7           4.9          1.8   Iris-virginica\n",
      "88            5.6          3.0           4.1          1.3  Iris-versicolor\n",
      "17            5.1          3.5           1.4          0.3      Iris-setosa\n",
      "94            5.6          2.7           4.2          1.3  Iris-versicolor\n"
     ]
    }
   ],
   "source": [
    "#randomize the indices\n",
    "indices = np.random.permutation(data.shape[0])\n",
    "div = int(0.75 * len(indices))\n",
    "development_id, test_id = indices[:div], indices[div:]\n",
    "#dividing the dataset using randomized indices\n",
    "development_set, test_set = data.loc[development_id,:], data.loc[test_id,:]\n",
    "print(\"Development Set:\\n\", development_set, \"\\n\\nTest Set:\\n\", test_set)\n",
    "mean_development_set = development_set.mean()\n",
    "mean_test_set = test_set.mean()\n",
    "std_development_set = development_set.std()\n",
    "std_test_set = test_set.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b)\n",
    "Implement kNN using the following hyperparameters:\n",
    "##### number of neighbor\n",
    "        * 1,3,5,7\n",
    "##### distance metric\n",
    "        * euclidean distance\n",
    "        * normalized euclidean distance\n",
    "        * cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving the 'class' column from the development and test sets and storing it in separate lists. Calculating the mean and standard deviation of the development set and test set for normalizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_class = list(test_set.iloc[:,-1])\n",
    "dev_class = list(development_set.iloc[:,-1])\n",
    "mean_development_set = development_set.mean()\n",
    "mean_test_set = test_set.mean()\n",
    "std_development_set = development_set.std()\n",
    "std_test_set = test_set.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for computing the Euclidean Distance, Normalized Euclidean Distance, Cosine Similarity and k Nearest Neighbor to determine the 'class' for a given input instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclideanDistance(data_1, data_2, data_len):\n",
    "    dist = 0\n",
    "    for i in range(data_len):\n",
    "        dist = dist + np.square(data_1[i] - data_2[i])\n",
    "    return np.sqrt(dist)\n",
    "\n",
    "def normalizedEuclideanDistance(data_1, data_2, data_len, data_mean, data_std):\n",
    "    n_dist = 0\n",
    "    for i in range(data_len):\n",
    "        n_dist = n_dist + (np.square(((data_1[i] - data_mean[i])/data_std[i]) - ((data_2[i] - data_mean[i])/data_std[i])))\n",
    "    return np.sqrt(n_dist)\n",
    "\n",
    "def cosineSimilarity(data_1, data_2):\n",
    "    dot = np.dot(data_1, data_2[:-1])\n",
    "    norm_data_1 = np.linalg.norm(data_1)\n",
    "    norm_data_2 = np.linalg.norm(data_2[:-1])\n",
    "    cos = dot / (norm_data_1 * norm_data_2)\n",
    "    return (1-cos)\n",
    "\n",
    "def knn(dataset, testInstance, k, dist_method, dataset_mean, dataset_std): \n",
    "    distances = {}\n",
    "    length = testInstance.shape[1]\n",
    "    if dist_method == 'euclidean':\n",
    "        for x in range(len(dataset)):\n",
    "            dist_up = euclideanDistance(testInstance, dataset.iloc[x], length)\n",
    "            distances[x] = dist_up[0]\n",
    "    elif dist_method == 'normalized_euclidean':\n",
    "        for x in range(len(dataset)):\n",
    "            dist_up = normalizedEuclideanDistance(testInstance, dataset.iloc[x], length, dataset_mean, dataset_std)\n",
    "            distances[x] = dist_up[0]\n",
    "    elif dist_method == 'cosine':\n",
    "        for x in range(len(dataset)):\n",
    "            dist_up = cosineSimilarity(testInstance, dataset.iloc[x])\n",
    "            distances[x] = dist_up[0]\n",
    "    # Sort values based on distance\n",
    "    sort_distances = sorted(distances.items(), key=operator.itemgetter(1))\n",
    "    neighbors = []\n",
    "    # Extracting nearest k neighbors\n",
    "    for x in range(k):\n",
    "        neighbors.append(sort_distances[x][0])\n",
    "    # Initializing counts for 'class' labels counts as 0\n",
    "    counts = {\"Iris-setosa\" : 0, \"Iris-versicolor\" : 0, \"Iris-virginica\" : 0}\n",
    "    # Computing the most frequent class\n",
    "    for x in range(len(neighbors)):\n",
    "        response = dataset.iloc[neighbors[x]][-1] \n",
    "        if response in counts:\n",
    "            counts[response] += 1\n",
    "        else:\n",
    "            counts[response] = 1\n",
    "    # Sorting the class in reverse order to get the most frequest class\n",
    "    sort_counts = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return(sort_counts[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c)\n",
    "Using the development data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating all of the development data points and computing the class for each k and each distance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EUCLIDEAN distance method performed on the dataset for all k values!\n",
      "NORMALIZED_EUCLIDEAN distance method performed on the dataset for all k values!\n",
      "COSINE distance method performed on the dataset for all k values!\n"
     ]
    }
   ],
   "source": [
    "# Creating a list of list of all columns except 'class' by iterating through the development set\n",
    "row_list = []\n",
    "for index, rows in development_set.iterrows():\n",
    "    my_list =[rows.sepal_length, rows.sepal_width, rows.petal_length, rows.petal_width]       \n",
    "    row_list.append([my_list])\n",
    "# k values for the number of neighbors that need to be considered\n",
    "k_n = [1, 3, 5, 7]\n",
    "# Distance metrics\n",
    "distance_methods = ['euclidean', 'normalized_euclidean', 'cosine']\n",
    "# Performing kNN on the development set by iterating all of the development set data points and for each k and each distance metric\n",
    "obs_k = {}\n",
    "for dist_method in distance_methods:\n",
    "    development_set_obs_k = {}\n",
    "    for k in k_n:\n",
    "        development_set_obs = []\n",
    "        for i in range(len(row_list)):\n",
    "            development_set_obs.append(knn(development_set, pd.DataFrame(row_list[i]), k, dist_method, mean_development_set, std_development_set))\n",
    "        development_set_obs_k[k] = development_set_obs\n",
    "    # Nested Dictionary containing the observed class for each k and each distance metric (obs_k of the form obs_k[dist_method][k])\n",
    "    obs_k[dist_method] = development_set_obs_k\n",
    "    print(dist_method.upper() + \" distance method performed on the dataset for all k values!\")\n",
    "#print(obs_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the accuracy for the development data set and finding the optimal hyperparametes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   k  euclidean  normalized_euclidean    cosine\n",
      "0  1   1.000000              1.000000  1.000000\n",
      "1  3   0.973214              0.964286  0.982143\n",
      "2  5   0.973214              0.964286  0.991071\n",
      "3  7   0.973214              0.982143  0.973214\n",
      "\n",
      "\n",
      "\n",
      "Best k value is\u001b[1m 5 \u001b[0mand best distance metric is\u001b[1m cosine \u001b[0m. Ignoring k=1 if the value of accuracy for k=1 is 100%, since this mostly implies overfitting\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculating the accuracy of the development set by comparing it with the development set 'class' list created earlier\n",
    "accuracy = {}\n",
    "for key in obs_k.keys():\n",
    "    accuracy[key] = {}\n",
    "    for k_value in obs_k[key].keys():\n",
    "        #print('k = ', key)\n",
    "        count = 0\n",
    "        for i,j in zip(dev_class, obs_k[key][k_value]):\n",
    "            if i == j:\n",
    "                count = count + 1\n",
    "            else:\n",
    "                pass\n",
    "        accuracy[key][k_value] = count/(len(dev_class))\n",
    "\n",
    "# Storing the accuracy for each k and each distance metric into a dataframe\n",
    "df_res = pd.DataFrame({'k': k_n})\n",
    "for key in accuracy.keys():\n",
    "    value = list(accuracy[key].values())\n",
    "    df_res[key] = value\n",
    "print(df_res)\n",
    "\n",
    "# Plotting a Bar Chart for accuracy\n",
    "draw = df_res.plot(x='k', y=['euclidean', 'normalized_euclidean', 'cosine'], kind=\"bar\", colormap='YlGnBu')\n",
    "draw.set(ylabel='Accuracy')\n",
    "\n",
    "# Ignoring k=1 if the value of accuracy for k=1 is 100%, since this mostly implies overfitting\n",
    "df_res.loc[df_res['k'] == 1.0, ['euclidean', 'normalized_euclidean', 'cosine']] = np.nan\n",
    "\n",
    "# Fetching the best k value for using all hyper-parameters\n",
    "# In case the accuracy is the same for different k and different distance metric selecting the first of all the same\n",
    "column_val = [c for c in df_res.columns if not c.startswith('k')]\n",
    "col_max = df_res[column_val].max().idxmax(1)\n",
    "best_dist_method = col_max\n",
    "row_max = df_res[col_max].argmax()\n",
    "best_k = int(df_res.iloc[row_max]['k'])\n",
    "if df_res.isnull().values.any():\n",
    "    print('\\n\\n\\nBest k value is\\033[1m', best_k, '\\033[0mand best distance metric is\\033[1m', best_dist_method, '\\033[0m. Ignoring k=1 if the value of accuracy for k=1 is 100%, since this mostly implies overfitting')\n",
    "else:\n",
    "    print('\\n\\n\\nBest k value is\\033[1m', best_k, '\\033[0mand best distance metric is\\033[1m', best_dist_method, '\\033[0m.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part d)\n",
    "Using the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Best k value is\u001b[1m 5 \u001b[0mand best distance metric is\u001b[1m cosine \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n\\nBest k value is\\033[1m', best_k, '\\033[0mand best distance metric is\\033[1m', best_dist_method, '\\033[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the best k value and best distance metric to determine the class for all rows in the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy of the Test dataset is  0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "# Creating a list of list of all columns except 'class' by iterating through the development set\n",
    "row_list_test = []\n",
    "for index, rows in test_set.iterrows(): \n",
    "    my_list =[rows.sepal_length, rows.sepal_width, rows.petal_length, rows.petal_width]       \n",
    "    row_list_test.append([my_list])\n",
    "test_set_obs = []\n",
    "for i in range(len(row_list_test)):\n",
    "    test_set_obs.append(knn(test_set, pd.DataFrame(row_list_test[i]), best_k, best_dist_method, mean_test_set, std_test_set))\n",
    "#print(test_set_obs)\n",
    "\n",
    "count = 0\n",
    "for i,j in zip(test_class, test_set_obs):\n",
    "    if i == j:\n",
    "        count = count + 1\n",
    "    else:\n",
    "        pass\n",
    "accuracy_test = count/(len(test_class))\n",
    "print('Final Accuracy of the Test dataset is ', accuracy_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
